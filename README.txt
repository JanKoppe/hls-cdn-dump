this is a half-incomplete dump of some way to set up a rather low-complexity HLS CDN.

this is not a plug-and-play solution. it's a dump of some custom setup that you can freely use for inspiration. the important parts (hls stream generation, cdn setup, metrics, terraform) have been used for several production events, but this is by no means something that you just spin up without understanding it.

the main functionality happens inside the Docker image, which is built around an ffmpeg process that will fetch some stream from an URL, e.g. some rtmp url. You can relay such an rtmp stream with the usual suspects: nginx-rtmp (easy to setup, but buggy/unstable) or srs (a tiny bit harder to setup due to lacking docs, but pretty stable). ffmpeg will automatically transcode to various bitrates & resolutions, chunk to HLS for easy deliverability and use the rotating AES key to encrypt the packages. If you want to put meaningful access control on your HLS stream while retaining the caching benefits of a CDN you could require some authentication scheme of your choice for accessing the AES decryption key. Additionally vector is deployed in the image as well to gather all the logs and push them into an S3 compatible server of your choice, e.g. to generate metrics or analyze errors.

the terraform code will spin up a hetzner VM with dedicated CPU cores and let it run this single Docker image. Additionally it will configure a bunnynet CDN to point to that origin server and cache the HLS chunks. You can achieve pretty high offload values with such a simple setup, having a few hundred viewers on this should be no issue. the CDN is also configured to stream logs to an rsyslog endpoint of your choice. an example config for vector to receive such logstreams and convert them into usable prometheus metrics is included in the bunny_metrics directory.
